{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clPcl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MT-Blachetta/clPcl_SingleGPU/blob/main/clPcl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYz7J1jFQOZc",
        "outputId": "251cd12d-04eb-4912-afd8-14e21eb7002b"
      },
      "source": [
        "!pip install spherecluster\n",
        "!pip install scikit-learn==0.20.0\n",
        "!wget https://github.com/MT-Blachetta/clPcl_SingleGPU/archive/refs/heads/main.zip\n",
        "!unzip main.zip\n",
        "!rm main.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spherecluster\n",
            "  Downloading spherecluster-0.1.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spherecluster) (3.6.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.4.1)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 40 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 51 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 92 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 154 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.0.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->spherecluster) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->spherecluster) (3.0.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (21.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.15.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (8.11.0)\n",
            "Installing collected packages: nose, spherecluster\n",
            "Successfully installed nose-1.3.7 spherecluster-0.1.7\n",
            "Collecting scikit-learn==0.20.0\n",
            "  Downloading scikit_learn-0.20.0-cp37-cp37m-manylinux1_x86_64.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.20.0) (1.4.1)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.0 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.20.0\n",
            "--2021-11-29 22:31:18--  https://github.com/MT-Blachetta/clPcl_pretextTask/archive/refs/heads/singleCudaGPU.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/MT-Blachetta/clPcl_pretextTask/zip/refs/heads/singleCudaGPU [following]\n",
            "--2021-11-29 22:31:18--  https://codeload.github.com/MT-Blachetta/clPcl_pretextTask/zip/refs/heads/singleCudaGPU\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘singleCudaGPU.zip’\n",
            "\n",
            "singleCudaGPU.zip       [  <=>               ]   2.76M  8.03MB/s    in 0.3s    \n",
            "\n",
            "2021-11-29 22:31:19 (8.03 MB/s) - ‘singleCudaGPU.zip’ saved [2893801]\n",
            "\n",
            "Archive:  singleCudaGPU.zip\n",
            "1835c2aa8a5ee542c9dff5df978be29267548470\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/LICENSE  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/README.md  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/RESULTS/\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/RESULTS/mnist/\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/RESULTS/mnist/pretext/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/RESULTS/mnist/pretext/checkpoint.pth.tar  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/RESULTS/mnist/pretext/model.pth.tar  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/clPcl.ipynb  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/configs/\n",
            " extracting: clPcl_pretextTask-singleCudaGPU/configs/env.yml  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/configs/pretext/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/pretext/clPcl_stl10.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/pretext/moco_imagenet100.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/pretext/moco_imagenet200.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/pretext/moco_imagenet50.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/pretext/simclr_cifar10.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/pretext/simclr_cifar20.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/pretext/simclr_mnist.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/pretext/simclr_stl10.yml  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/configs/scan/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/scan/imagenet_eval.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/scan/scan_cifar10.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/scan/scan_cifar20.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/scan/scan_imagenet_100.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/scan/scan_imagenet_200.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/scan/scan_imagenet_50.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/scan/scan_mnist.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/scan/scan_stl10.yml  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/configs/selflabel/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/selflabel/selflabel_cifar10.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/selflabel/selflabel_cifar20.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/selflabel/selflabel_imagenet_100.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/selflabel/selflabel_imagenet_200.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/selflabel/selflabel_imagenet_50.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/selflabel/selflabel_mnist.yml  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/configs/selflabel/selflabel_stl10.yml  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/data/\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/data/__pycache__/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/__pycache__/augment.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/__pycache__/custom_dataset.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/__pycache__/mnist.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/__pycache__/stl.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/augment.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/cifar.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/custom_dataset.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/imagenet.py  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/data/imagenet_subsets/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/imagenet_subsets/imagenet_100.txt  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/imagenet_subsets/imagenet_200.txt  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/imagenet_subsets/imagenet_50.txt  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/mnist.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/data/stl.py  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/dataset/\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/dataset/MNIST/\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/dataset/MNIST/processed/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/dataset/MNIST/processed/test.pt  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/images/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/images/illustration.jpg  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/logs/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/logs/scan_stl10.txt  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/losses/\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/losses/__pycache__/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/losses/__pycache__/losses.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/losses/losses.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/moco.py  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/models/\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/models/__pycache__/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/__pycache__/mnet.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/__pycache__/models.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/__pycache__/resnet_stl.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/clPcl_model.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/mnet.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/models.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/resnet.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/resnet_cifar.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/models/resnet_stl.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/pcPcl_execution.ipynb  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/pcl_cld.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/requirements.txt  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/run_pcPcl.ipynb  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/scan.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/selflabel.py  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/utils/\n",
            "   creating: clPcl_pretextTask-singleCudaGPU/utils/.ipynb_checkpoints/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/.ipynb_checkpoints/MemoryBank_example-checkpoint.ipynb  \n",
            "   creating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/\n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/collate.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/common_config.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/config.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/evaluate_utils.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/memory.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/mypath.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/train_utils.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/__pycache__/utils.cpython-37.pyc  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/collate.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/common_config.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/config.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/ema.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/evaluate_utils.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/memory.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/mypath.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/train_utils.py  \n",
            "  inflating: clPcl_pretextTask-singleCudaGPU/utils/utils.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEn8wY6ZJItE"
      },
      "source": [
        "#--config_env configs/env.yml \n",
        "#--config_exp configs/pretext/clPcl_stl10.yml\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from utils.config import create_config\n",
        "from utils.common_config import get_criterion, get_backbone_model , get_instance_model,get_group_model, get_train_dataset,\\\n",
        "                                get_val_dataset, get_train_dataloader,\\\n",
        "                                get_val_dataloader, get_train_transformations,\\\n",
        "                                get_val_transformations, get_optimizer,\\\n",
        "                                adjust_learning_rate, get_clustering\n",
        "from utils.evaluate_utils import contrastive_evaluate\n",
        "from utils.memory import MemoryBank\n",
        "from utils.train_utils import pcl_cld_train\n",
        "from utils.utils import fill_memory_bank\n",
        "from termcolor import colored"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge-VUFnklnzS",
        "outputId": "9475fadd-eaa8-48cc-d98e-1a6f27deb914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "    #1# Retrieve config file\n",
        "p = create_config(\"configs/env.yml\", \"configs/pretext/clPcl_stl10.yml\")\n",
        "print(colored(p, 'red'))\n",
        "print(colored('Retrieve model', 'blue'))\n",
        "    \n",
        "backbone = get_backbone_model(p)\n",
        "print('Model is {}'.format(backbone.__class__.__name__))\n",
        "#print('Model parameters: {:.2f}M'.format(sum(p.numel() for p in backbone.parameters()) / 1e6))\n",
        "print(backbone)\n",
        "    \n",
        "    \n",
        "instance_model = get_instance_model(p, backbone)\n",
        "instance_head = instance_model.get_head()\n",
        "    \n",
        "group_model = get_group_model(p, backbone)\n",
        "group_head = group_model.get_head()\n",
        "backbone_model = group_model.get_backbone()\n",
        "print('Model is {}'.format(instance_model.__class__.__name__))\n",
        "print('Model parameters: {:.2f}M'.format(sum(p.numel() for p in instance_model.parameters()) / 1e6))\n",
        "print(instance_model)\n",
        "print('Model is {}'.format(group_model.__class__.__name__))\n",
        "print('Model parameters: {:.2f}M'.format(sum(p.numel() for p in group_model.parameters()) / 1e6))\n",
        "print(group_model)\n",
        "    #instance_model = instance_model.cuda()\n",
        "    #group_model = group_model.cuda()\n",
        "   \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m{'setup': 'clPcl', 'clustering': [2, 4, 8, 16], 'backbone': 'resnet18', 'model_kwargs': {'head': 'linear', 'features_dim': 128}, 'train_db_name': 'stl-10', 'val_db_name': 'stl-10', 'num_classes': 10, 'criterion': 'clPcl', 'criterion_kwargs': {'temperature': 0.1}, 'epochs': 500, 'optimizer': 'sgd', 'optimizer_kwargs': {'nesterov': False, 'weight_decay': 0.0001, 'momentum': 0.9, 'lr': 0.4}, 'scheduler': 'cosine', 'scheduler_kwargs': {'lr_decay_rate': 0.1}, 'batch_size': 512, 'num_workers': 0, 'augmentation_strategy': 'simclr', 'augmentation_kwargs': {'random_resized_crop': {'size': 96, 'scale': [0.2, 1.0]}, 'color_jitter_random_apply': {'p': 0.8}, 'color_jitter': {'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4, 'hue': 0.1}, 'random_grayscale': {'p': 0.2}, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'transformation_kwargs': {'crop_size': 96, 'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, 'pretext_dir': 'RESULTS/stl-10/pretext', 'pretext_checkpoint': 'RESULTS/stl-10/pretext/checkpoint.pth.tar', 'pretext_checkpoint_instance': 'RESULTS/stl-10/pretext/instance_checkpoint.pth.tar', 'pretext_checkpoint_group': 'RESULTS/stl-10/pretext/group_checkpoint.pth.tar', 'pretext_model': 'RESULTS/stl-10/pretext/model.pth.tar', 'pretext_model_instance': 'RESULTS/stl-10/pretext/instance_model.pth.tar', 'pretext_model_group': 'RESULTS/stl-10/pretext/group_model.pth.tar', 'topk_neighbors_train_path': 'RESULTS/stl-10/pretext/topk-train-neighbors.npy', 'topk_neighbors_val_path': 'RESULTS/stl-10/pretext/topk-val-neighbors.npy'}\u001b[0m\n",
            "\u001b[34mRetrieve model\u001b[0m\n",
            "Model is dict\n",
            "{'backbone': ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
            "), 'dim': 512}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1c5bbb228c4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minstance_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_instance_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0minstance_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mgroup_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_group_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MoCo' object has no attribute 'get_head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coZEx4q7tBAT"
      },
      "source": [
        "     #> CUDNN\n",
        "print(colored('Set CuDNN benchmark', 'blue')) \n",
        "torch.backends.cudnn.benchmark = True    \n",
        "    ###\n",
        "\n",
        "\n",
        "#3# Dataset                                                       OK\n",
        "    #A - get transformormations for the dataset\n",
        "print(colored('Retrieve dataset', 'blue'))\n",
        "train_transforms = get_train_transformations(p) \n",
        "print('Train transforms:', train_transforms)\n",
        "val_transforms = get_val_transformations(p)\n",
        "    \n",
        "    #B - get Dataset from files\n",
        "print('Validation transforms:', val_transforms)\n",
        "split_ = 'train'\n",
        "\n",
        "if p['train_db_name'] == 'stl-10': \n",
        "  split_ = 'train+unlabeled'\n",
        "\n",
        "train_dataset = get_train_dataset(p, train_transforms, to_augmented_dataset=True,\n",
        "                                        split=split_) # Split is for stl-10\n",
        "                                        \n",
        "val_dataset = get_val_dataset(p, val_transforms)\n",
        "    \n",
        "#C - put the dataset to the dataloader for training purposes\n",
        "train_dataloader = get_train_dataloader(p, train_dataset)\n",
        "val_dataloader = get_val_dataloader(p, val_dataset)\n",
        "print('Dataset contains {}/{} train/val samples'.format(len(train_dataset), len(val_dataset)))\n",
        "        #4# Memory Bank\n",
        "print(colored('Build MemoryBank', 'blue'))\n",
        "base_dataset = get_train_dataset(p, val_transforms, split='train') # Dataset w/o augs for knn eval\n",
        "base_dataloader = get_val_dataloader(p, base_dataset) \n",
        "    \n",
        "memory_bank_base = MemoryBank(len(base_dataset), \n",
        "                                p['model_kwargs']['features_dim'],\n",
        "                                p['num_classes'], p['criterion_kwargs']['temperature'])\n",
        "\n",
        "    \n",
        "memory_bank_val = MemoryBank(len(val_dataset),\n",
        "                                p['model_kwargs']['features_dim'],\n",
        "                                p['num_classes'], p['criterion_kwargs']['temperature'])\n",
        "\n",
        "iloss = torch.nn.CrossEntropyLoss()\n",
        "iloss.cuda()\n",
        "\n",
        "# Criterion\n",
        "print(colored('Retrieve criterion', 'blue'))\n",
        "criterion = get_criterion(p)\n",
        "print('Criterion is {}'.format(criterion.__class__.__name__))\n",
        "criterion = criterion.cuda()\n",
        "\n",
        "    # Optimizer and scheduler                                       \n",
        "print(colored('Retrieve optimizer', 'blue'))\n",
        "optimizer = get_optimizer(p, group_model)\n",
        "print(optimizer)\n",
        "    ###\n",
        "M_num_clusters = get_clustering(p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnxfd2FkuE0M"
      },
      "source": [
        "    #6# Checkpoint to continue last training phase                             OK\n",
        "if os.path.exists(p['pretext_checkpoint_backbone']):\n",
        "  print(colored('Restart from checkpoint (backbone) {}'.format(p['pretext_checkpoint_backbone']), 'blue'))\n",
        "  checkpoint = torch.load(p['pretext_checkpoint_backbone'], map_location='cpu')\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  backbone_model.load_state_dict(checkpoint['model'])\n",
        "\tinstance_model.set_backbone(backbone_model)\n",
        "\tgroup_model.set_backbone(backbone_model)\n",
        "        #backbone.cuda()\n",
        "  start_epoch = checkpoint['epoch']\n",
        "\n",
        "else:\n",
        "  print(colored('No checkpoint file at {}'.format(p['pretext_checkpoint']), 'blue'))\n",
        "  start_epoch = 0\n",
        "      \n",
        "  \n",
        "if os.path.exists(p['pretext_checkpoint_instance']):\n",
        "  print(colored('Restart from checkpoint (instance_model) {}'.format(p['pretext_checkpoint_instance']), 'blue'))\n",
        "  checkpoint = torch.load(p['pretext_checkpoint_instance'], map_location='cpu')\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  instance_head.load_state_dict(checkpoint['model'])  \n",
        "        \n",
        "\tinstance_model.set_head(instance_head)\n",
        "  instance_model = instance_model.cuda()\n",
        "  start_epoch = checkpoint['epoch']\n",
        "\n",
        "else:\n",
        "  print(colored('No checkpoint file at {}'.format(p['pretext_checkpoint']), 'blue'))\n",
        "  start_epoch = 0\n",
        "  instance_model = instance_model.cuda()\n",
        "        \n",
        "if os.path.exists(p['pretext_checkpoint_group']):\n",
        "  print(colored('Restart from checkpoint (group_model) {}'.format(p['pretext_checkpoint_group']), 'blue'))\n",
        "  checkpoint = torch.load(p['pretext_checkpoint_group'], map_location='cpu')\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  group_head.load_state_dict(checkpoint['model'])\n",
        "\t\n",
        "\tgroup_model.set_head(group_head)\n",
        "  group_model = group_model.cuda()\n",
        "  start_epoch = checkpoint['epoch']\n",
        "        \n",
        "else:\n",
        "  print(colored('No checkpoint file at {}'.format(p['pretext_checkpoint']), 'blue'))\n",
        "  start_epoch = 0\n",
        "  group_model = group_model.cuda()\n",
        "        \n",
        "    ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5bP-h63weKS"
      },
      "source": [
        "print(colored('Starting main loop', 'blue'))\n",
        "\n",
        "for epoch in range(start_epoch, p['epochs']):\n",
        "  print(colored('Epoch %d/%d' %(epoch, p['epochs']), 'yellow'))\n",
        "  print(colored('-'*15, 'yellow'))\n",
        "\n",
        "        #a - Adjust lr\n",
        "  lr = adjust_learning_rate(p, optimizer, epoch)\n",
        "  print('Adjusted learning rate to {:.5f}'.format(lr))\n",
        "        \n",
        "        #b - Train the model with the clPcl method for one epoch (iteration)\n",
        "  print('Train ...')\n",
        "  pcl_cld_train(train_loader = train_dataloader, instance_branch = instance_model, group_branch = group_model, criterion = criterion, optimizer = optimizer, epoch = epoch, M_num_clusters = M_num_clusters)\n",
        "\n",
        "        #c - Fill memory bank (Data Structure for nearest neighbors of input-instances)\n",
        "  print('Fill memory bank for kNN...')\n",
        "  fill_memory_bank(base_dataloader, group_model, memory_bank_base)\n",
        "\n",
        "        #d - Evaluate (To monitor progress - Not for validation)\n",
        "  print('Evaluate ...')\n",
        "  top1 = contrastive_evaluate(val_dataloader, group_model, memory_bank_base)\n",
        "  print('Result of kNN evaluation is %.2f' %(top1)) \n",
        "        \n",
        "        #e - Checkpoint\n",
        "  print('Checkpoint ...')\n",
        "        \n",
        "  torch.save({'optimizer': optimizer.state_dict(), 'model': group_model.get_backbone().state_dict(), \n",
        "                    'epoch': epoch + 1}, p['pretext_checkpoint_backbone'])\n",
        "        \n",
        "  torch.save({'optimizer': optimizer.state_dict(), 'model': instance_model.get_head().state_dict(), \n",
        "                    'epoch': epoch + 1}, p['pretext_checkpoint_instance'])\n",
        "                    \n",
        "  torch.save({'optimizer': optimizer.state_dict(), 'model': group_model.get_head().state_dict(), \n",
        "                    'epoch': epoch + 1}, p['pretext_checkpoint_group'])\n",
        "                    \n",
        "        \n",
        "\n",
        "    # Save final model\n",
        "  torch.save(instance_model.state_dict(), p['pretext_model_instance'])\n",
        "  torch.save(group_model.state_dict(),p['pretext_model_group'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycvf_AMnwYfh"
      },
      "source": [
        "# single run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzR7syJ3mURp",
        "outputId": "53694ff8-479b-4ff7-f69f-70d0f78265a3"
      },
      "source": [
        "        #7# Training\n",
        "print(colored('Starting main loop', 'blue'))\n",
        "#for epoch in range(start_epoch, 1):\n",
        "    #print(colored('Epoch %d/%d' %(epoch, p['epochs']), 'yellow'))\n",
        "    #print(colored('-'*15, 'yellow'))\n",
        "\n",
        "        #a - Adjust lr\n",
        "lr = adjust_learning_rate(p, optimizer, 1)\n",
        "print('Adjusted learning rate to {:.5f}'.format(lr))\n",
        "        \n",
        "        #b - Train the model with the clPcl method for one epoch (iteration)\n",
        "print('Train ...')\n",
        "#def pcl_cld_train(args, train_loader, instance_branch, group_branch, criterion, optimizer, epoch, M_num_clusters):\n",
        "import torch\n",
        "import numpy as np\n",
        "#from spherecluster import VonMisesFisherMixture\n",
        "import nltk\n",
        "from nltk.cluster.kmeans import KMeansClusterer\n",
        "from utils.utils import AverageMeter, ProgressMeter\n",
        "#backbone = get_backbone_model(p)\n",
        "#instance_model = get_instance_model(p, backbone)\n",
        "#group_model = get_group_model(p, backbone)\n",
        "#losses = AverageMeter('Loss', ':.4e')\n",
        "#progress = ProgressMeter(len(train_dataloader),[losses],prefix=\"Epoch: [{}]\".format(1))\n",
        "        \n",
        "instance_model.train()\n",
        "group_model.train()\n",
        "#instance_model = instance_model.cuda()\n",
        "#group_model = group_model.cuda()\n",
        "print(\"initialized pcl_cld_train\")\n",
        "\n",
        "i = 1\n",
        "batch = next(iter(train_dataloader))\n",
        "originImage_batch = batch['image']\n",
        "augmentedImage_batch = batch['image_augmented']\n",
        "originImage_batch = originImage_batch.cuda(non_blocking=True)\n",
        "augmentedImage_batch = augmentedImage_batch.cuda(non_blocking=True)\n",
        "print(\"batch_image_shape: \"+str(originImage_batch.shape))\n",
        "type(batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34mStarting main loop\u001b[0m\n",
            "Adjusted learning rate to 0.40000\n",
            "Train ...\n",
            "initialized pcl_cld_train\n",
            "batch_image_shape: torch.Size([64, 3, 96, 96])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6xdUb00WEVz",
        "outputId": "0fbcc4ad-f8df-4379-d233-31f4332f56bc"
      },
      "source": [
        "\n",
        "#print('Model is {}'.format(backbone.__class__.__name__))\n",
        "#print('Model parameters: {:.2f}M'.format(sum(p.numel() for p in backbone.parameters()) / 1e6))\n",
        "#print(backbone)\n",
        "#originImage_batch.cuda()\n",
        "#augmentedImage_batch.cuda()\n",
        "\n",
        "logits, labels = instance_model(originImage_batch,augmentedImage_batch)\n",
        "logits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IN MoCo->_dequeue_and_enqueue: batch_size = 64\n",
            "queue_ptr = 0\n",
            "next ptr = 64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[13.6168,  1.0472,  0.1281,  ..., -0.1120,  0.1565, -1.8276],\n",
              "        [13.7274,  1.5451, -0.0224,  ..., -0.9390, -0.2487, -1.7784],\n",
              "        [13.8654,  2.0759,  0.0908,  ..., -1.5972, -0.1473, -2.2851],\n",
              "        ...,\n",
              "        [12.9380,  1.9026,  0.2363,  ..., -1.4277,  0.0797, -1.6944],\n",
              "        [13.8260,  1.3726,  0.0316,  ..., -0.5932, -0.0624, -1.8486],\n",
              "        [13.8354,  1.0272,  0.1188,  ..., -0.5746,  0.0777, -2.1932]],\n",
              "       device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6BUjioDf-H6",
        "outputId": "678461a7-3ed2-49ad-8dc5-356540886408"
      },
      "source": [
        "#import torch.nn.functional as F\n",
        "#labels.cuda() \n",
        "#instance_loss = F.cross_entropy(logits,labels)\n",
        "instance_loss = iloss(logits,labels)\n",
        "instance_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO_p4xMI5GeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42413f2-8309-4d4c-e382-0220d536c69d"
      },
      "source": [
        "original_view = group_model(originImage_batch)\n",
        "augmented_view = group_model(augmentedImage_batch)\n",
        "\n",
        "M_kmeans_results = []\n",
        "MI_kmeans_results = []\n",
        "concentration_matrices = []\n",
        "concentration_matrices_I = []\n",
        "M_labels = []\n",
        "M_labels_I = []\n",
        "\n",
        "feature_dim = len(original_view[0])\n",
        "batch_size = len(original_view)\n",
        "\n",
        "# ov = original_view.cpu().detach().numpy()\n",
        "# av = augmented_view.cpu().detach().numpy()\n",
        "print(feature_dim)\n",
        "print(batch_size)\n",
        "print(original_view.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "64\n",
            "torch.Size([64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myM-ssTh83ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ff9a80-33b3-4d67-81b2-b7f1ef54642c"
      },
      "source": [
        "from spherecluster import SphericalKMeans\n",
        "from utils.utils import AverageMeter, ProgressMeter\n",
        "import copy\n",
        "\n",
        "alpha = 0.1\n",
        "divzero = 0.1\n",
        "ov = original_view.cpu().detach().numpy()\n",
        "print(ov.shape)\n",
        "av = augmented_view.cpu().detach().numpy()\n",
        "\n",
        "for k in M_num_clusters:\n",
        "  #from spherecluster import SphericalKMeans\n",
        "  skm = SphericalKMeans(n_clusters=k)\n",
        "  skm.fit(ov)\n",
        "  skm_I = SphericalKMeans(n_clusters=k)\n",
        "  skm_I.fit(av)\n",
        "\n",
        "  M_kmeans_results.append(torch.Tensor(skm.cluster_centers_))\n",
        "  MI_kmeans_results.append(torch.Tensor(skm_I.cluster_centers_))\n",
        "            # c -> k\n",
        "  center = [ torch.Tensor(skm.cluster_centers_[i]) for i in range(len(skm.cluster_centers_)) ]\n",
        "  center_I = [ torch.Tensor(skm_I.cluster_centers_[i]) for i in range(len(skm_I.cluster_centers_)) ]\n",
        "  cdat = [ x.unsqueeze(0).expand(batch_size,feature_dim) for x in center]\n",
        "  cmatrix = torch.cat(cdat,1)\n",
        "  cdat_I = [ x.unsqueeze(0).expand(batch_size,feature_dim) for x in center_I]\n",
        "  cmatrix_I = torch.cat(cdat_I,1)\n",
        "\n",
        "  original_cpu = original_view.cpu()\n",
        "  augmented_cpu = augmented_view.cpu()          \n",
        "  fmatrix = torch.Tensor(copy.deepcopy(ov))\n",
        "  fmatrix_I = torch.Tensor(copy.deepcopy(av))\n",
        "  #fmatrix = copy.deepcopy(original_cpu)\n",
        "  #fmatrix_I = copy.deepcopy(augmented_cpu)\n",
        "\n",
        "  for _ in range(1,k): fmatrix = torch.cat((fmatrix,original_cpu),1)\n",
        "  for _ in range(1,k): fmatrix_I = torch.cat((fmatrix_I,augmented_cpu),1)\n",
        "                \n",
        "  cmatrix = cmatrix.cuda()\n",
        "  fmatrix = fmatrix.cuda()\n",
        "  cmatrix_I = cmatrix_I.cuda()\n",
        "  fmatrix_I = fmatrix_I.cuda()\n",
        "            \n",
        "  zmatrix = fmatrix-cmatrix\n",
        "  zmatrix = zmatrix*zmatrix\n",
        "  result = zmatrix.flatten(0).view(batch_size,k,feature_dim)\n",
        "  result = torch.sum(result,2)\n",
        "\n",
        "  zmatrix_I = fmatrix_I-cmatrix_I\n",
        "  zmatrix_I = zmatrix_I*zmatrix_I\n",
        "  result_I = zmatrix_I.flatten(0).view(batch_size,k,feature_dim)\n",
        "  result_I = torch.sum(result_I,2)\n",
        "  assign = torch.zeros(batch_size,k)\n",
        "  assign_I = torch.zeros(batch_size,k)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    assign[i][ int(skm.labels_[i]) ] = 1\n",
        "    assign_I[i][ int(skm_I.labels_[i]) ] = 1\n",
        "                \n",
        "  assign = assign.cuda()\n",
        "  assign_I = assign_I.cuda()\n",
        "            \n",
        "  avgDistance = torch.sum(assign*result,0)\n",
        "  Z = torch.sum(assign,0) + 1\n",
        "  Zlog = torch.log(Z+alpha)\n",
        "  divisor = Z*Zlog\n",
        "  concentrations = (avgDistance/divisor) + divzero\n",
        "  concentrations = concentrations.cpu()\n",
        "            #avgDistance = avgDistance.cuda()\n",
        "            #divisor = divisor.cuda()\n",
        "  avgDistance_I = torch.sum(assign_I*result_I,0)\n",
        "  Z_I = torch.sum(assign_I,0) + 1\n",
        "  Zlog_I = torch.log(Z_I+alpha)\n",
        "  divisor_I = Z_I*Zlog_I\n",
        "  concentrations_I = (avgDistance_I/divisor_I) + divzero\n",
        "  concentrations_I = concentrations_I.cpu()\n",
        "            \n",
        "  concentration_matrices.append(concentrations)\n",
        "  concentration_matrices_I.append(concentrations_I)\n",
        "            \n",
        "  M_labels.append( skm.labels_ )\n",
        "  M_labels_I.append( skm_I.labels_ )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XptHZM6yKJPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b43978-6269-4f49-bcfb-75f40175f4a6"
      },
      "source": [
        "# concentration_matrices - list of Tensors\n",
        "# M_kmeans_results - list of Tensors\n",
        "# M_labels - list of numpy.ndarray's\n",
        "# features = original view = cuda-Tensor\n",
        "concentration_matrices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0.1138, 0.1156], grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.1164, 0.1114, 0.1126, 0.1159], grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.1115, 0.1150, 0.1143, 0.1157, 0.1159, 0.1136, 0.1106, 0.1000],\n",
              "        grad_fn=<ToCopyBackward0>),\n",
              " tensor([0.1106, 0.1120, 0.1000, 0.1139, 0.1095, 0.1096, 0.1072, 0.1093, 0.1132,\n",
              "         0.1000, 0.1133, 0.1000, 0.1103, 0.1000, 0.1127, 0.1000],\n",
              "        grad_fn=<ToCopyBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJXxpwbEP58Y"
      },
      "source": [
        "# **Group Loss (debugging)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHdO5P6SAE8i",
        "outputId": "f7e8b59c-6d74-4ae3-8d89-605cff8949f8"
      },
      "source": [
        "#group_loss = criterion\n",
        "#import math\n",
        "\n",
        "features = original_view \n",
        "features_I = augmented_view \n",
        "M_kmeans = M_kmeans_results  \n",
        "M_kmeans_I = MI_kmeans_results \n",
        "concentrations = concentration_matrices \n",
        "concentrations_I = concentration_matrices_I \n",
        "labels = M_labels \n",
        "labels_I = M_labels_I\n",
        "lb = 1\n",
        "M_num = len(concentrations)\n",
        "print(M_num)\n",
        "batch_size = features.size()[0]\n",
        "#batch_size = original_view.size()[0]\n",
        "M_kmeans = M_kmeans_results \n",
        "M_kmeans_I = MI_kmeans_results\n",
        "\n",
        "M_logits = []\n",
        "M_logits_I = []\n",
        "\n",
        "#if k == 2: print()\n",
        "        \n",
        "for k in range(M_num):\n",
        "  c = len(concentrations[k]) # c = num_clusters of Mk\n",
        "  M_cmatrix = torch.zeros(c,batch_size)\n",
        "  MI_cmatrix = torch.zeros(c,batch_size)\n",
        "  for i in range(c):\n",
        "    M_cmatrix[i,:] = 1/concentrations[k][i]\n",
        "    MI_cmatrix[i,:] = 1/concentrations_I[k][i]\n",
        "\n",
        "  #if k == 2: print(M_cmatrix)          \n",
        "  M_cmatrix = M_cmatrix.cuda()\n",
        "  MI_cmatrix = MI_cmatrix.cuda()     \n",
        "  centroids = M_kmeans[k].cuda()\n",
        "  centroids_I = M_kmeans_I[k].cuda()\n",
        "  gLoss_or = torch.mm(centroids,features_I.T) # OK \n",
        "  gLoss_au = torch.mm(centroids_I,features.T)\n",
        "  #gLoss_or = torch.mm(centroids,augmented_view.T) # OK \n",
        "  #gLoss_au = torch.mm(centroids_I,original_view.T)\n",
        "  print(\"gLoss_or type: \"+str(type(gLoss_or)) )\n",
        "  print(\"gLoss_or shape: \"+str(gLoss_or.shape) )\n",
        "#--------------------------------------------------------\n",
        "  summing_logits = gLoss_or * M_cmatrix # OK\n",
        "  summing_logits_I = gLoss_au * MI_cmatrix\n",
        "            \n",
        "  exp_logits = torch.exp(summing_logits)\n",
        "  exp_logits_I = torch.exp(summing_logits_I)\n",
        "  log_sum = torch.sum(exp_logits,0)\n",
        "  print(\"log_sum type: \"+str(type(log_sum)))\n",
        "  print(\"log_sum shape: \"+str(log_sum.shape))\n",
        "  log_sum_I = torch.sum(exp_logits_I,0)\n",
        "    \n",
        "  positive_pair = torch.zeros(batch_size)\n",
        "  positive_pair_I = torch.zeros(batch_size)\n",
        "  \n",
        "  exlogCPU = exp_logits.cpu()\n",
        "  exlogCPU_I = exp_logits_I.cpu()\n",
        "  #lcpu = labels[k].cuda()\n",
        "  #lcpu_ = labels_I[k].cuda()\n",
        "  for l in range(batch_size):\n",
        "    positive_pair[l] = exlogCPU[int(labels[k][l])][l]\n",
        "    positive_pair_I[l] = exlogCPU_I[int(labels_I[k][l])][l]\n",
        "  \n",
        "  positive_pair = positive_pair.cuda()\n",
        "  positive_pair_I = positive_pair_I.cuda()\n",
        "            #positive_pair = torch.exp(torch.mm(positive_pair,gLoss_or))\n",
        "            #positive_pair_I = torch.exp(torch.mm(positive_pair_I,gLoss_au))\n",
        "            \n",
        "  M_logits.append( torch.sum( torch.log(positive_pair/log_sum) ).cpu() ) # +0.0001 ),0).cpu()       ) # check type shape size and len !!!!!!\n",
        "  M_logits_I.append( torch.sum( torch.log(positive_pair_I/log_sum_I) ).cpu() ) # +0.0001 ),0).cpu() ) \n",
        "            \n",
        "result = lb*(-1/M_num)*0.5*( sum(M_logits) + sum(M_logits_I) )\n",
        "\n",
        "# if math.isnan(result): result = 1000000\n",
        "loss = instance_loss + result\n",
        "\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "gLoss_or type: <class 'torch.Tensor'>\n",
            "gLoss_or shape: torch.Size([2, 64])\n",
            "log_sum type: <class 'torch.Tensor'>\n",
            "log_sum shape: torch.Size([64])\n",
            "gLoss_or type: <class 'torch.Tensor'>\n",
            "gLoss_or shape: torch.Size([4, 64])\n",
            "log_sum type: <class 'torch.Tensor'>\n",
            "log_sum shape: torch.Size([64])\n",
            "gLoss_or type: <class 'torch.Tensor'>\n",
            "gLoss_or shape: torch.Size([8, 64])\n",
            "log_sum type: <class 'torch.Tensor'>\n",
            "log_sum shape: torch.Size([64])\n",
            "gLoss_or type: <class 'torch.Tensor'>\n",
            "gLoss_or shape: torch.Size([16, 64])\n",
            "log_sum type: <class 'torch.Tensor'>\n",
            "log_sum shape: torch.Size([64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(107.0621, grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMZZyysSZUes",
        "outputId": "746c5826-9b94-4565-c5b3-341e3953a693"
      },
      "source": [
        "#import math\n",
        "#if math.isnan(result): print(\"hit\")\n",
        "\n",
        "#M_logits[3]\n",
        "gLoss_or[15]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7720, 0.8382, 0.9848, 0.9325, 0.9402, 0.9391, 0.9725, 0.7768, 0.9377,\n",
              "        0.9297, 0.9822, 0.7676, 0.8109, 0.8661, 0.9301, 0.8629, 0.9385, 0.9184,\n",
              "        0.9419, 0.8366, 0.9571, 0.9364, 0.7582, 0.9660, 0.9743, 0.8421, 0.8107,\n",
              "        0.9721, 0.8074, 0.9561, 0.8316, 0.6752, 0.8958, 0.6500, 0.8663, 0.9596,\n",
              "        0.9185, 0.8886, 0.9598, 0.6563, 0.9769, 0.8240, 0.9089, 0.7385, 0.7065,\n",
              "        0.7262, 0.9192, 0.9111, 0.7214, 0.9165, 0.9661, 0.8357, 0.8414, 0.7493,\n",
              "        0.9021, 0.9420, 0.8866, 0.9535, 0.9483, 0.8401, 0.8741, 0.9685, 0.9406,\n",
              "        0.9285], device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnMUU93g77ql"
      },
      "source": [
        "#losses.update(loss.item())\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "#if i % 25 == 0:\n",
        "#  progress.display(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK15HBx2_qR3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}